---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: single
#classes: wide
author_profile: true
title: My digital CV
permalink: /cv/
toc: true
toc_label: BARTCUS Marius
toc_sticky: true
---

# Short bio

I come from a beautiful eastern Europe country Moldova, where I finished my licence in **software engineering**. By the end, I got experience in developing desktop and web applications. I was very passionate but I decided to move to France. Here, I obtained a master degree in Computer Science, that is in **Data mining and exploration EID2**, at University Paris 13 and than a **Ph.D** in **Statistical Learning** at University of Toulon. At the end, I had different opportunities to gain experience in academic and company environments. Today, I am working on the 11 projects proposed by the Open Classroom course for the **Data Scientist : Artificial Intelligence Engineer** in order to improve some of my skills.

My professional interests are in ***artificial intelligence*** üß† : ***machine learning***, ***deep learning*** techniques and their text/visio/audio applications.

My hobbies are: sport (running üèÉÔ∏è, foot, chess), travelüß≥, movies/series.

In the weekend I am the happiest men spending time with my family - my wife and our boys üë®‚Äçüë©‚Äçüë¶‚Äçüë¶.

You can get my short [CV](assets/docs/CV_Bartcus.pdf) in a pdf format or you can consult it more detailed in a digital form on this page.

Looking forward üëÄ to get in touch with you and working on some interesting projects.


# Experiences

<table>
    <tr>
        <td width="15%">Period</td>
        <td width="5%">Title</td>
        <td width="20%">Place</td>
        <td width="60%">Description</td>
    </tr>


    <tr>
        <td>Sep 2020 ‚Äì Feb 2022</td>
        <td>PostDoc researcher </td>
        <td>Centre L√©on Berard & INSA, Lyon, France</td>
        <td>
        Automatic segmentation of 3D lung nodules through deep learning. Lung cancer is one of the leading causes of cancer death worldwide. Characterization of lung tumors should be done by machine learning using radiomics techniques. Therefore we need to automatically segment the lung nodules in CT images. To segment lung tumors, we use deep learning which gives good results in terms of quality, robustness and computation time.
        </td>
    </tr>


    <tr>
        <td>Sep 2018 ‚Äì Feb 2020</td>
        <td>Research Engineer in Data science</td>
        <td>University of Caen, Caen France</td>
        <td>
        The task was to participate to the creation of a data science platform dedicated to unsupervised classification of high-dimensional data. The first step concerns prototyping developed algorithms by the members of the project AStERiCs, their integration into this platform on various real applications. These are unsupervised classification algorithms based on latent variable models. The second step is to integrate the algorithms that will be developed during the project. A part of the work will be carried out in collaboration with the LMNO lab on distributed regularized mixture models with environmental applications / genomic sequences. The main missions are in Developing unsupervised learning models, Prototyping unsupervised learning algorithms, High performance distributed cloud computing and Web integration and interfacing with the platform.</td>
    </tr>


    <tr>
        <td>Mar 2017 ‚Äì Feb 2018</td>
        <td>Engineer R&D</td>
        <td>Orange Labs, Lanion, France</td>
        <td>Co-clustering is a data mining technique that aims at identifying the underlying structure between the rows and the columns of a data matrix in the form of homogeneous blocks. It finds many real world applications, however many current co-clustering algorithms are not suited on large size data set. One of the successfully used approach to co-cluster big sized data reaching to millions of instances and tens of thousands of values per dimension is the MODL co-clustering method that optimizes a criterion based on a regularized likelihood. However, difficulties are encountered with up to billions of values per variable. This post-doc focuses on developing a co-clustering algorithm, given the MODL criterion allowing to efficiently deal with very large data sets that does not fit in memory. My work was to co-cluster large scale data sets in a reasonable time by using less RAM memory then the existing co-clustering techniques. Real world data sets with variable pairs of values Texts-Words, Source-Target web sites, User-Films.</td>
    </tr>

    <tr>
        <td>Dec 2015 ‚Äì Jul 2016</td>
        <td>PostDoc researcher</td>
        <td>Toulon University, Toulon France</td>
        <td>I worked on the two applications that are Humpback whale song structuration and Birds classification. For the humpback structuration was to segment the signal to give different hypothesis of these song. Bayesian non-parametric learning approaches are used to automatically decompose the whale signal and produce song units that are considered as kind of whale alphabet. For the second application the task was to classify the bird songs in an automatic way to identify bird species in some recording. Also in a context when several species are present in a recording, the goal is to find the foreground specie (the one that appears closes to the microphone), this induced to a single label classification problem.</td>
    </tr>

    <tr>

        <td>Sep 2011 ‚Äì Oct 2015</td>
        <td>Ph.D researcher</td>
        <td>Toulon University, Toulon France</td>
        <td>
        This thesis focuses on statistical learning and multi-dimensional data analysis. It particularly focuses on unsupervised learning of generative models for model-based clustering. We study the Gaussians mixture models, in the context of maximum likelihood estimation via the EM algorithm, as well as in the Bayesian estimation context by maximum a posteriori via Markov Chain Monte Carlo (MCMC) sampling techniques. We mainly consider the parsimonious mixture models which are based on a spectral decomposition of the covariance matrix and provide a flexible framework particularly for the analysis of high-dimensional data. Then, we investigate non-parametric Bayesian mixtures which are based on general flexible processes such as the Dirichlet process and the Chinese Restaurant Process. This non-parametric model formulation is relevant for both learning the model, as well for dealing with the issue of model selection. We propose new Bayesian non-parametric parsimonious mixtures and derive a MCMC sampling technique where the mixture model and the number of mixture components are simultaneously learned from the data. The selection of the model structure is performed by using Bayes Factors. These models, by their non-parametric and sparse formulation, are useful for the analysis of large data sets when the number of classes is undetermined and increases with the data, and when the dimension is high. The models are validated on simulated data and standard real data sets. Then, they are applied to a real difficult problem of automatic structuring of complex bioacoustic data issued from whale song signals. Finally, we open Markovian perspectives via hierarchical Dirichlet processes hidden Markov models.
        </td>

    </tr>
</table>
